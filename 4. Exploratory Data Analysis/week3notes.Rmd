---
title: "Week 3 Notes"
author: "CJ Arayata"
date: "October 31, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# hierarchical clustering

how  to define what is close, what is not, what defines a group and how to interpret?

hierarchical: find two things, put them together, find next closest
-need defined distance, end up with tree showing how close things are

Garbage in -> garbage out

Distance, or similarity? Continuous is euclidian or correlation, binary is manhattan

Euclidian is as the crow flies, manhattan is how many 'blocks' you're covering

# Merging points

When you take two points and call them the same group, what is their new center?

```{r cars}
x <- rnorm(100, 10, 1)
y <- rnorm(100, 15, 1)
data <- data.frame(x = x, y = y)

distance <- dist(data)

clustering <- hclust(distance)

plot(clustering)

heatmap(as.matrix(data))
```

# Issues

Gives idea, but for exploratory only. Picture can be unstable based on points, missing values, different distance, merging strategy

But is deterministic. Where to cut isn't always obvious

# K means clustering

Can we find things that are close together?

Partitioning approach: fix k-number of clusters, get 'centroids', assign things to closest centroid, recalculate centroids

Requires defined distance metric, number of clusters, initial guess as to centroids

Produces: Final estimate of centroids, cluster assignments

```{r}
set.seed(1234)

x <- rnorm(12, rep(1:3, each = 4), 0.2)
y <- rnorm(12, rep(c(1, 2, 1), each = 4), 0.2)

plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))

data <- data.frame(x, y)
kmeans.obj <- kmeans(data, centers = 3)

# let's plot the points and centroids
plot(x, y, col = kmeans.obj$cluster, pch = 19, cex = 2)
points(kmeans.obj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)

```

# Notes

It requires a number of clusters, so not deterministic and you can fiddle with it. Also, the number of iterations as well

# PCA and Singular Value Decomposition

You've got a bunch of predictors. Find a smaller set that are uncorrelated and explain as much variance as possible. Statistical goal, but also data compression

SVD: x  = UDV(transposed)
columns of U are left singular vectors
columnds of V are right singular vectors
D is diagonal matrix of singular values

singular value squared / sum of singular values, squared = proportion variance explained
PCA: principal components equal to the right singular values if you first scale the variables (subtract mean, divide by SD)

Missing values are always a thing, needs to be a complete matrix!
```{r}
library(impute)

impute.knn(matrix.with.mising.values)
```

# Plotting and Color

Defaults are bad. Use colors well cause they can help!

default heat.colors() and topo.colors() are OK

```{r}

# helps interpolate between colors
colorRamp() # take a palette and return a function that takes values between 0 and 1
colorRampPalette() # take integer arguments and return vector of colors
pal <- colorRamp(c("red", "blue"))

pal(0)
# 255    0    0
pal(.5)
# 127.5    0 127.5
pal(.75)
# 63.75    0 191.25
pal(1)
# 0    0  255

```

RColorBrewer is cool, sequential, diverging, and qualitative

rgb takes three args between 0-1 to create any color
Transparency via alpha parameter to rgb
